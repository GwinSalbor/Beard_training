{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'imgaug'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-849d0520db61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mimgaug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmenters\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0miaa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimgaug\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'imgaug'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Conv2D, MaxPooling2D, BatchNormalization, \\\n",
    "    SeparableConv2D, GlobalAveragePooling2D\n",
    "from keras.utils import to_categorical, Sequence\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras import initializers\n",
    "from numpy.random import seed\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "import imgaug.augmenters as iaa\n",
    "import imgaug as ia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = \"./dataset/labels_for_test.json\"\n",
    "with open(labels) as label:\n",
    "    data = json.load(label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare(labels):\n",
    "    data = {}\n",
    "    for key in labels.keys():\n",
    "        if labels[key] not in data:\n",
    "            data.update({labels[key]: []})\n",
    "        data[labels[key]].append(key)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = data_prepare(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "train_data = {}\n",
    "val_data = {}\n",
    "for i in data:\n",
    "    if len(data[i]) > max_len:\n",
    "        max_len = len(data[i])\n",
    "    train_data.update({i: data[i][:int(0.9*len(data[i]))]})\n",
    "    val_data.update({i: data[i][int(0.9*len(data[i])):]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 256, 256\n",
    "\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5),\n",
    "    sometimes(iaa.Affine(\n",
    "    rotate=(-45, 45),\n",
    ")),\n",
    "])\n",
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "\n",
    "for label in train_data:\n",
    "    if not os.path.exists('dataset/learning_data/' + label):\n",
    "        os.makedirs('dataset/learning_data/' + label)\n",
    "    for i, img_name in enumerate(train_data[label]):\n",
    "        image = cv2.imread('dataset/data_for_test/' +  img_name)\n",
    "        gray = cv2.resize(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY), (img_width, img_height))\n",
    "        cv2.imwrite('dataset/learning_data/' + label + \"/\" + str(i) +\".jpg\", gray)\n",
    "    for i in range(max_len-len(train_data[label])):\n",
    "        image = cv2.imread('dataset/data_for_test/' + random.choice(train_data[label]))\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        images_aug = seq(images=gray)\n",
    "        cv2.imwrite('dataset/learning_data/' + label + \"/\" + str(len(train_data[label]) + i) +\".jpg\", images_aug)\n",
    "\n",
    "for label in val_data:\n",
    "    if not os.path.exists('dataset/validation_data/' + label):\n",
    "        os.makedirs('dataset/validation_data/' + label)\n",
    "    for i, img_name in enumerate(val_data[label]):\n",
    "        image = cv2.imread('dataset/data_for_test/' +  img_name)\n",
    "        gray = cv2.resize(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY), (img_width, img_height))\n",
    "        cv2.imwrite('dataset/validation_data/' + label + \"/\" + str(i) +\".jpg\", gray)\n",
    "        \n",
    "del(val_data)\n",
    "del(train_data)\n",
    "del(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = \"dataset/learning_data/\"\n",
    "validation_data_dir = \"dataset/validation_data/\"\n",
    "nb_train_samples = 20\n",
    "nb_validation_samples = 2\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "base_model = applications.VGG16(weights='imagenet', include_top=False)\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu',\n",
    "          kernel_initializer=initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None))(x)\n",
    "\n",
    "predictions = Dense(8, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq(Sequence):\n",
    "    def __init__(self, data, batch_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.data[idx * self.batch_size:(idx + 1) * self.batch_size][\"image\"]\n",
    "        batch_y = self.data[idx * self.batch_size:(idx + 1) * self.batch_size][\"label\"]\n",
    "\n",
    "        return (np.array([cv2.resize(cv2.imread(file_name, 1), (256, 256)) for file_name in batch_x]),\n",
    "                keras.utils.to_categorical(np.array(batch_y), num_classes =8))\n",
    "\n",
    "def build_model():\n",
    "    input_img = Input(shape=(256,256,3), name='ImageInput')\n",
    "    x = Conv2D(64, (3,3), activation='relu', padding='same', name='Conv1_1')(input_img)\n",
    "    x = Conv2D(64, (3,3), activation='relu', padding='same', name='Conv1_2')(x)\n",
    "    x = MaxPooling2D((2,2), name='pool1')(x)\n",
    "    \n",
    "    x = SeparableConv2D(128, (3,3), activation='relu', padding='same', name='Conv2_1')(x)\n",
    "    x = SeparableConv2D(128, (3,3), activation='relu', padding='same', name='Conv2_2')(x)\n",
    "    x = MaxPooling2D((2,2), name='pool2')(x)\n",
    "    \n",
    "    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_1')(x)\n",
    "    x = BatchNormalization(name='bn1')(x)\n",
    "    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_2')(x)\n",
    "    x = BatchNormalization(name='bn2')(x)\n",
    "    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_3')(x)\n",
    "    x = MaxPooling2D((2,2), name='pool3')(x)\n",
    "    \n",
    "    x = SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_1')(x)\n",
    "    x = BatchNormalization(name='bn3')(x)\n",
    "    x = SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_2')(x)\n",
    "    x = BatchNormalization(name='bn4')(x)\n",
    "    x = SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_3')(x)\n",
    "    x = MaxPooling2D((2,2), name='pool4')(x)\n",
    "    \n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(1024, activation='relu', name='fc1')(x)\n",
    "    x = Dropout(0.1, name='dropout1')(x)\n",
    "    x = Dense(512, activation='relu', name='fc2')(x)\n",
    "    x = Dropout(0.1, name='dropout2')(x)\n",
    "    x = Dense(8, activation='softmax', name='fc3')(x)\n",
    "    \n",
    "    model = Model(inputs=input_img, outputs=x)\n",
    "    return model\n",
    "model =  build_model()\n",
    "\n",
    "model =  build_model()\n",
    "\n",
    "opt = Adam(lr=0.0001, decay=1e-5)\n",
    "es = EarlyStopping(patience=5)\n",
    "chkpt = ModelCheckpoint(filepath='best_model_todate', save_best_only=True, save_weights_only=True)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=opt)\n",
    "\n",
    "batch_size = 8\n",
    "nb_epochs = 20\n",
    "\n",
    "train_data_gen = Seq(data=train_data, batch_size=batch_size)\n",
    "\n",
    "nb_train_steps = train_data.shape[0]//batch_size\n",
    "\n",
    "history = model.fit_generator(generator=train_data_gen, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n",
    "                               validation_data=(valid_data, valid_labels),callbacks=[es, chkpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
